{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83051515-128f-43d2-991a-ab518db718d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Databricks Platform Overview\n",
    "This project aims to load the raw data from the wind turbines into Databricks, process & transform, and provide a platform for analytics on this data.\n",
    "\n",
    "##Database Objects\n",
    "We use a Unity Catalog enabled Databricks workspace, with a catalog created for this project `lakehouse_sbx`. Under this is a dedicated schema for the project called `cd_edw`. This is a managed schema located on a dedicated storage account container and will serve as a central location for all objects created as part of this project.\n",
    "\n",
    "The project also follows the medallion architecture of Bronze -> Silver -> Gold where possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d65fdd68-d120-4f6d-81c1-657404605c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Ingest Raw Data\n",
    "Ingest raw CSV data from Azure Data Lake. This location is saved and exposed as a volume on Databricks Unity Catalog under the name `/Volumes/lakehouse_sbx/cd_edw/landing`.\n",
    "\n",
    "We specify the schema to avoid issues in the future, but also include a `_rescued_data` column to catch any new fields that may appear in the future.\n",
    "\n",
    "Since the source data is an append-only set of CSV files that is updated once daily, the following options were considered:\n",
    "1. Batch loads on a trigger everyday after the files are updated to load all raw data into a Dataframe and overwrite the Bronze table.\n",
    "1. Set up a Python method to track the rows that were read for each file, and every subsequent load would skip the previously loaded lines in the CSV and load only the new records (if any exist). \n",
    "    - This essentially decouples the integration layer with the data load layer, allowing the Databricks pipeline to be run whenever and at any frequency, regardless of when data is landed in the data lake.\n",
    "    - Reduces overhead in the future as the data grows as data is always incrementally processed.\n",
    "    - Caveat, this adds a bit of complexity to the pipeline, with an audit log Delta Table and reading from and writing to this table during each data load run.\n",
    "\n",
    "##Questions\n",
    "1. Do we have control over the ingest pattern to create new files that are date-partitioned instead of appending to a single/group of files? This could allow for using Autoloader, which handles checkpoints and processed files automatically enabling incremental loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aae9c114-e1d8-43f8-9751-a556c41e9f38",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Audit Log table"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS lakehouse_sbx.cd_edw.edw_turbine_ingest_audit_log (\n",
    "    file_name STRING,\n",
    "    file_path STRING,\n",
    "    last_processed_position LONG,\n",
    "    process_time TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4c38f8-88f2-47d3-846f-7c9ca0f70adc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType, DecimalType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"timestamp\", TimestampType(), True),\n",
    "  StructField(\"turbine_id\", IntegerType(), True),\n",
    "  StructField(\"wind_speed\", DecimalType(6, 2), True),\n",
    "  StructField(\"wind_direction\", DecimalType(6, 2), True),\n",
    "  StructField(\"power_output\", DecimalType(6, 2), True)\n",
    "])\n",
    "\n",
    "def load_csv_incremental(schema, file_path):\n",
    "    file_path_list = dbutils.fs.ls(file_path)\n",
    "\n",
    "    final_df = spark.createDataFrame([], schema=schema)\n",
    "\n",
    "    for file in file_path_list:\n",
    "        last_position = spark.sql(f\"\"\"\n",
    "                SELECT COALESCE(MAX(last_processed_position), 0) as pos\n",
    "                FROM lakehouse_sbx.cd_edw.edw_turbine_ingest_audit_log\n",
    "                WHERE file_name = '{file.name}'\n",
    "            \"\"\").collect()[0]['pos']\n",
    "        \n",
    "        temp_df = (spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", True) \\\n",
    "            .option(\"mode\", \"PERMISSIVE\") \\\n",
    "            .option(\"rescuedDataColumn\", \"_rescued_data\") \\\n",
    "            .option(\"skipRows\", last_position)\n",
    "            .schema(schema) \\\n",
    "            .load(f\"{file_path}{file.name}\") \\\n",
    "            .select(\n",
    "                \"*\"\n",
    "                , col(\"_metadata.file_path\").alias(\"raw_file_path\")\n",
    "                , element_at(split(\"_metadata.file_path\", '/'), array_size(split(\"_metadata.file_path\", '/'))).alias(\"raw_file_name\")\n",
    "                , from_utc_timestamp(current_timestamp(), 'GMT').alias(\"bronze_processing_time\")\n",
    "            ))\n",
    "                \n",
    "        if temp_df.count() > 0:\n",
    "            temp_df.write \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(\"`lakehouse_sbx`.`cd_edw`.`brnz_turbine_data`\",mode=\"append\")\n",
    "\n",
    "            new_position = last_position + temp_df.count()\n",
    "            spark.sql(f\"\"\"\n",
    "                INSERT INTO lakehouse_sbx.cd_edw.edw_turbine_ingest_audit_log \n",
    "                VALUES (\n",
    "                    '{file.name}'\n",
    "                    , '{file_path}'\n",
    "                    , '{new_position}'\n",
    "                    , current_timestamp()\n",
    "                )\n",
    "            \"\"\")\n",
    "\n",
    "load_csv_incremental(schema, '/Volumes/lakehouse_sbx/cd_edw/landing/cd_raw_data/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03cd39ca-9719-4296-9452-46ecaf2ae3bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Create Transformation Tables for Silver Layer\n",
    "Create all tables required for cleaning and transforming Turbine data. Performs the following steps:\n",
    "1. Create temporary dataframe with all 24h for the last 30d at the time of processing. This will be used to impute any missing values.\n",
    "1. Join this against the Bronze table to get a \"master\" list of all datapoints over the last 30 days, with readings as NULLs for where they are missing.\n",
    "1. Impute these missing values from the previous known readings. A simple `last` function is used to forward fill values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f55f88-c106-4a1d-bfd8-b4c2a41d5dd8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clean and Impute"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "##########################\n",
    "# Load delta table into df\n",
    "##########################\n",
    "data_df = spark.read.format(\"delta\") \\\n",
    "                    .table(\"lakehouse_sbx.cd_edw.brnz_turbine_data\") \\\n",
    "                    .select(\"timestamp\", \"turbine_id\", \"wind_speed\", \"wind_direction\", \"power_output\")\n",
    "\n",
    "impute_table_name = \"lakehouse_sbx.cd_edw.silver_impute_turbine_data\"\n",
    "min_date, max_date = data_df.select(min(\"timestamp\"), max(\"timestamp\")).first()\n",
    "table_exists = spark.catalog.tableExists(impute_table_name)\n",
    "\n",
    "if table_exists:\n",
    "    min_date = max_date - timedelta(days=30)\n",
    "\n",
    "hourly_range = pd.date_range(start=min_date, end=max_date, freq='H')\n",
    "\n",
    "all_hours = spark.createDataFrame(\n",
    "    [(ts.to_pydatetime(),) for ts in hourly_range],\n",
    "    [\"timestamp\"]\n",
    ")\n",
    "\n",
    "all_turbines = data_df.select(\"turbine_id\").distinct()\n",
    "\n",
    "complete_index = all_hours.crossJoin(all_turbines)\n",
    "\n",
    "window_spec = Window.partitionBy(\"turbine_id\").orderBy(\"timestamp\").rowsBetween(-24, 0)\n",
    "\n",
    "impute_df = complete_index.join(data_df, [\"timestamp\", \"turbine_id\"], \"left\") \\\n",
    "    .withColumn(\"wind_speed_filled\", last(\"wind_speed\", ignorenulls=True).over(window_spec)) \\\n",
    "    .withColumn(\"wind_direction_filled\", last(\"wind_direction\", ignorenulls=True).over(window_spec)) \\\n",
    "    .withColumn(\"power_output_filled\", last(\"power_output\", ignorenulls=True).over(window_spec))\n",
    "    \n",
    "final_filled_df = impute_df \\\n",
    "    .withColumn(\"wind_speed\", coalesce(\"wind_speed\", \"wind_speed_filled\")) \\\n",
    "    .withColumn(\"wind_direction\", coalesce(\"wind_direction\", \"wind_direction_filled\")) \\\n",
    "    .withColumn(\"power_output\", coalesce(\"power_output\", \"power_output_filled\")) \\\n",
    "    .drop(\"wind_speed_filled\", \"wind_direction_filled\", \"power_output_filled\")\n",
    "\n",
    "if not table_exists:\n",
    "    final_filled_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"turbine_id\", \"date\") \\\n",
    "        .saveAsTable(impute_table_name)\n",
    "else:\n",
    "    target_table = DeltaTable.forName(spark, impute_table_name)\n",
    "    (\n",
    "        target_table.alias(\"target\")\n",
    "        .merge(\n",
    "            final_filled_df.alias(\"source\"),\n",
    "            \"target.timestamp = source.timestamp AND target.turbine_id = source.turbine_id\"\n",
    "        )\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1a911c-6250-4157-86fc-b0404d7ab881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stat01_table_name = \"lakehouse_sbx.cd_edw.silver_turbine_statistics_01\"\n",
    "table_exists = spark.catalog.tableExists(stat01_table_name)\n",
    "data_df = spark.read.format(\"delta\") \\\n",
    "                    .table(\"lakehouse_sbx.cd_edw.silver_impute_turbine_data\") \\\n",
    "                    .select(\"timestamp\", \"turbine_id\", \"wind_speed\", \"wind_direction\", \"power_output\")\n",
    "\n",
    "trfn_df = data_df.withColumn(\"date\", to_date(col(\"timestamp\"))) \\\n",
    "            .withColumn(\"month\", month(col(\"timestamp\"))) \\\n",
    "            .withColumn(\"24h_avg_output\", avg(\"power_output\").over(Window.partitionBy(\"turbine_id\", \"date\")).cast(\"decimal(6,2)\")) \\\n",
    "            .withColumn(\"24h_stddev_output\", stddev(\"power_output\").over(Window.partitionBy(\"turbine_id\", \"date\")).cast(\"decimal(7,3)\")) \\\n",
    "            .withColumn(\"1m_avg_output\", avg(\"power_output\").over(Window.partitionBy(\"turbine_id\", \"month\")).cast(\"decimal(6,2)\")) \\\n",
    "            .withColumn(\"1m_stddev_output\", stddev(\"power_output\").over(Window.partitionBy(\"turbine_id\", \"month\")).cast(\"decimal(7,3)\")) \\\n",
    "            .na.fill({\"power_output\": 0.0, \"wind_speed\": 0.0}) \\\n",
    "            .withColumn(\"is_24h_anomaly\", (col(\"power_output\") > (col(\"24h_avg_output\") + (2 * col(\"24h_stddev_output\"))))) \\\n",
    "            .withColumn(\"silver_processing_time\", from_utc_timestamp(current_timestamp(), 'GMT'))\n",
    "\n",
    "trfn_df.sort(\"timestamp\",\"turbine_id\") \\\n",
    "    .where(trfn_df.turbine_id == 1) \\\n",
    "    .display()\n",
    "\n",
    "# if not table_exists:\n",
    "#     trfn_df.write.format(\"delta\") \\\n",
    "#         .mode(\"overwrite\") \\\n",
    "#         .saveAsTable(stat01_table_name)\n",
    "# else:\n",
    "#     target_table = DeltaTable.forName(spark, stat01_table_name)\n",
    "#     (\n",
    "#         target_table.alias(\"target\")\n",
    "#         .merge(\n",
    "#             trfn_df.alias(\"source\"),\n",
    "#             \"target.timestamp = source.timestamp AND target.turbine_id = source.turbine_id\"\n",
    "#         )\n",
    "#         .whenMatchedUpdateAll()\n",
    "#         .whenNotMatchedInsertAll()\n",
    "#         .execute()\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f02f17-3ad5-48a8-a774-b5df54db866d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_filled_df.sort(\"turbine_id\", \"timestamp\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30cea029-b9f0-46aa-b324-5e1f7d2ebf8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from lakehouse_sbx.cd_edw.brnz_turbine_data\n",
    "where turbine_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc7b527-ed64-427b-a5f7-0fb5718e5f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "impute_table_name = \"lakehouse_sbx.cd_edw.brnz_turbine_data\"\n",
    "a = spark.catalog.tableExists(impute_table_name)\n",
    "# Determine full or incremental load\n",
    "# try:\n",
    "#     spark.catalog.tableExists(impute_table_name)\n",
    "# except AnalysisException:\n",
    "#     table_exists = False\n",
    "\n",
    "print(a)\n",
    "# # Date range for imputation\n",
    "# min_ts, max_ts = data_df.select(min(\"timestamp\"), max(\"timestamp\")).first()\n",
    "\n",
    "# if table_exists:\n",
    "#     min_date = max_ts - timedelta(days=30)\n",
    "# else:\n",
    "#     min_date = min_ts  # Use full range on first load"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6870650516407928,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Load_Transform_Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
