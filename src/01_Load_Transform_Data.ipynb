{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83051515-128f-43d2-991a-ab518db718d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Databricks Platform Overview\n",
    "This project aims to load the raw data from the wind turbines into Databricks, process & transform, and provide a platform for analytics on this data.\n",
    "\n",
    "##Database Objects\n",
    "We use a Unity Catalog enabled Databricks workspace, with a catalog created for this project `lakehouse_sbx`. Under this is a dedicated schema for the project called `cd_edw`. This is a managed schema located on a dedicated storage account container and will serve as a central location for all objects created as part of this project.\n",
    "\n",
    "The project also follows the medallion architecture of Bronze -> Silver -> Gold where possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d65fdd68-d120-4f6d-81c1-657404605c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Ingest Raw Data\n",
    "Ingest raw CSV data from Azure Data Lake. This location is saved and exposed as a volume on Databricks Unity Catalog under the name `/Volumes/lakehouse_sbx/cd_edw/landing`.\n",
    "\n",
    "We specify the schema to avoid issues in the future, but also include a `_rescued_data` column to catch any new fields that may appear in the future.\n",
    "\n",
    "Since the source data is an append-only set of CSV files that is updated once daily, the following options were considered:\n",
    "1. Batch loads on a trigger everyday after the files are updated to load all raw data into a Dataframe and overwrite the Bronze table.\n",
    "1. Set up a Python method to track the rows that were read for each file, and every subsequent load would skip the previously loaded lines in the CSV and load only the new records (if any exist). \n",
    "    - This essentially decouples the integration layer with the data load layer, allowing the Databricks pipeline to be run whenever and at any frequency, regardless of when data is landed in the data lake.\n",
    "    - Reduces overhead in the future as the data grows as data is always incrementally processed.\n",
    "    - Caveat, this adds a bit of complexity to the pipeline, with an audit log Delta Table and reading from and writing to this table during each data load run.\n",
    "\n",
    "###Questions\n",
    "1. Do we have control over the ingest pattern to create new files that are date-partitioned instead of appending to a single/group of files? This could allow for using Autoloader, which handles checkpoints and processed files automatically enabling incremental loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "707f38d7-6416-441e-b3b0-d049b3a6d32b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType, DecimalType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"timestamp\", TimestampType(), True),\n",
    "  StructField(\"turbine_id\", IntegerType(), True),\n",
    "  StructField(\"wind_speed\", DecimalType(6, 2), True),\n",
    "  StructField(\"wind_direction\", DecimalType(6, 2), True),\n",
    "  StructField(\"power_output\", DecimalType(6, 2), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aae9c114-e1d8-43f8-9751-a556c41e9f38",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Audit Log table"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS lakehouse_sbx.cd_edw.edw_turbine_ingest_audit_log (\n",
    "    file_name STRING,\n",
    "    file_path STRING,\n",
    "    last_processed_position LONG,\n",
    "    process_time TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4c38f8-88f2-47d3-846f-7c9ca0f70adc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Bronze Table"
    }
   },
   "outputs": [],
   "source": [
    "def load_csv_incremental(schema, file_path):\n",
    "    file_path_list = dbutils.fs.ls(file_path)\n",
    "\n",
    "    final_df = spark.createDataFrame([], schema=schema)\n",
    "\n",
    "    for file in file_path_list:\n",
    "        last_position = spark.sql(f\"\"\"\n",
    "                SELECT COALESCE(MAX(last_processed_position), 0) as pos\n",
    "                FROM lakehouse_sbx.cd_edw.edw_turbine_ingest_audit_log\n",
    "                WHERE file_name = '{file.name}'\n",
    "            \"\"\").collect()[0]['pos']\n",
    "        \n",
    "        temp_df = (spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", True) \\\n",
    "            .option(\"mode\", \"PERMISSIVE\") \\\n",
    "            .option(\"rescuedDataColumn\", \"_rescued_data\") \\\n",
    "            .option(\"skipRows\", last_position)\n",
    "            .schema(schema) \\\n",
    "            .load(f\"{file_path}{file.name}\") \\\n",
    "            .select(\n",
    "                \"*\"\n",
    "                , col(\"_metadata.file_path\").alias(\"raw_file_path\")\n",
    "                , element_at(split(\"_metadata.file_path\", '/'), array_size(split(\"_metadata.file_path\", '/'))).alias(\"raw_file_name\")\n",
    "                , from_utc_timestamp(current_timestamp(), 'GMT').alias(\"bronze_processing_time\")\n",
    "            ))\n",
    "                \n",
    "        if temp_df.count() > 0:\n",
    "            temp_df.write \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(\"`lakehouse_sbx`.`cd_edw`.`brnz_turbine_data`\",mode=\"append\")\n",
    "\n",
    "            new_position = last_position + temp_df.count()\n",
    "            spark.sql(f\"\"\"\n",
    "                INSERT INTO lakehouse_sbx.cd_edw.edw_turbine_ingest_audit_log \n",
    "                VALUES (\n",
    "                    '{file.name}'\n",
    "                    , '{file_path}'\n",
    "                    , '{new_position}'\n",
    "                    , current_timestamp()\n",
    "                )\n",
    "            \"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03cd39ca-9719-4296-9452-46ecaf2ae3bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create Transformation Tables for Silver Layer\n",
    "Create all tables required for cleaning and transforming Turbine data. Performs the following steps:\n",
    "1. Create temporary dataframe with all 24h for the last 30d at the time of processing. This will be used to impute any missing values.\n",
    "1. Join this against the Bronze table to get a \"master\" list of all datapoints over the last 30 days, with readings as NULLs for where they are missing.\n",
    "1. Impute these missing values from the previous known readings. A simple `last` function is used to forward fill values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f55f88-c106-4a1d-bfd8-b4c2a41d5dd8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clean and Impute"
    }
   },
   "outputs": [],
   "source": [
    "def mergeIntoDeltaTable(df, table_exists, table_name):\n",
    "    if not table_exists:\n",
    "        df.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .clusterBy(\"turbine_id\", \"date\") \\\n",
    "            .saveAsTable(table_name)\n",
    "    else:\n",
    "        target_table = DeltaTable.forName(spark, table_name)\n",
    "        (\n",
    "            target_table.alias(\"target\")\n",
    "            .merge(\n",
    "                df.alias(\"source\"),\n",
    "                \"target.timestamp = source.timestamp AND target.turbine_id = source.turbine_id\"\n",
    "            )\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "def imputeMissingData():\n",
    "    data_df = spark.read.format(\"delta\") \\\n",
    "                        .table(\"lakehouse_sbx.cd_edw.brnz_turbine_data\") \\\n",
    "                        .select(\"timestamp\", \"turbine_id\", \"wind_speed\", \"wind_direction\", \"power_output\")\n",
    "\n",
    "    impute_table_name = \"lakehouse_sbx.cd_edw.slvr_turbine_data_01\"\n",
    "    min_date, max_date = data_df.select(min(\"timestamp\"), max(\"timestamp\")).first()\n",
    "    table_exists = spark.catalog.tableExists(impute_table_name)\n",
    "\n",
    "    if table_exists:\n",
    "        min_date = max_date - timedelta(days=10)\n",
    "\n",
    "    hourly_range = pd.date_range(start=min_date, end=max_date, freq='H')\n",
    "\n",
    "    all_hours = spark.createDataFrame(\n",
    "        [(ts.to_pydatetime(),) for ts in hourly_range],\n",
    "        [\"timestamp\"]\n",
    "    )\n",
    "\n",
    "    all_turbines = data_df.select(\"turbine_id\").distinct()\n",
    "\n",
    "    complete_index = all_hours.crossJoin(all_turbines)\n",
    "\n",
    "    window_spec = Window.partitionBy(\"turbine_id\").orderBy(\"timestamp\").rowsBetween(-24, 0)\n",
    "\n",
    "    final_filled_df = complete_index.join(broadcast(data_df), [\"timestamp\", \"turbine_id\"], \"left\") \\\n",
    "        .withColumn(\"wind_speed\", coalesce(\"wind_speed\", last(\"wind_speed\", ignorenulls=True).over(window_spec))) \\\n",
    "        .withColumn(\"wind_direction\", coalesce(\"wind_direction\", last(\"wind_direction\", ignorenulls=True).over(window_spec))) \\\n",
    "        .withColumn(\"power_output\", coalesce(\"power_output\", last(\"power_output\", ignorenulls=True).over(window_spec))) \\\n",
    "        .withColumn(\"date\", to_date(col(\"timestamp\"))) \\\n",
    "        .withColumn(\"month\", month(col(\"timestamp\"))) \\\n",
    "\n",
    "    final_filled_df = final_filled_df.repartition(\"turbine_id\", \"date\")\n",
    "\n",
    "    mergeIntoDeltaTable(final_filled_df, table_exists, impute_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a8a8c1b-e022-44d7-8296-0c6df0d53b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Calculate statistics and transformations\n",
    "Use the cleaned and filled in data from above and calculate 24h and 30d averages and standard deviations.\n",
    "\n",
    "###Assumptions\n",
    "The following assumptions are made here:\n",
    "1. Anomalies are calculated over a sliding 24h window, as well as a calendar day window. These are created as flags that can be filtered on.\n",
    "1. For each window, the averages and standard deviations are calculated, and anomalies are where the reading is 2x stddev above the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1a911c-6250-4157-86fc-b0404d7ab881",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Stats Table"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.window import Window\n",
    "\n",
    "def calculateStats():\n",
    "    stats_table_name = \"lakehouse_sbx.cd_edw.slvr_turbine_data_02\"\n",
    "    table_exists = spark.catalog.tableExists(stats_table_name)\n",
    "    data_df = spark.read.format(\"delta\") \\\n",
    "                        .table(\"lakehouse_sbx.cd_edw.slvr_turbine_data_01\") \\\n",
    "                        .select(\"timestamp\", \"date\", \"month\", \"turbine_id\", \"wind_speed\", \"wind_direction\", \"power_output\")\n",
    "\n",
    "    window_daily = Window.partitionBy(\"turbine_id\", \"date\")\n",
    "    window_24h = Window.partitionBy(\"turbine_id\").orderBy(\"timestamp\").rowsBetween(-24, 0)\n",
    "    window_monthly = Window.partitionBy(\"turbine_id\", \"month\")\n",
    "\n",
    "    stats_df = data_df \\\n",
    "                .withColumn(\"day_avg_output\", avg(\"power_output\").over(window_daily).cast(\"decimal(6,2)\")) \\\n",
    "                .withColumn(\"day_stddev_output\", stddev(\"power_output\").over(window_daily).cast(\"decimal(7,3)\")) \\\n",
    "                .withColumn(\"24h_avg_output\", avg(\"power_output\").over(window_24h).cast(\"decimal(6,2)\")) \\\n",
    "                .withColumn(\"24h_stddev_output\", stddev(\"power_output\").over(window_24h).cast(\"decimal(7,3)\")) \\\n",
    "                .withColumn(\"is_24h_anomaly\", coalesce((col(\"power_output\") > (col(\"24h_avg_output\") + (2 * col(\"24h_stddev_output\")))), lit(False))) \\\n",
    "                .withColumn(\"is_day_anomaly\", coalesce((col(\"power_output\") > (col(\"day_avg_output\") + (2 * col(\"day_stddev_output\")))), lit(False))) \\\n",
    "                .withColumn(\"silver_processing_time\", from_utc_timestamp(current_timestamp(), 'GMT'))\n",
    "\n",
    "    stats_df = stats_df.repartition(\"turbine_id\", \"date\")\n",
    "\n",
    "    # stats_df.sort(\"timestamp\",\"turbine_id\") \\\n",
    "    #     .where(stats_df.turbine_id == 1) \\\n",
    "    #     .display()\n",
    "\n",
    "    mergeIntoDeltaTable(stats_df, table_exists, stats_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fab4eb3-c7c3-40d0-b79c-2734a0b1ce9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create Aggregated Tables\n",
    "Use the silver tables from above to create and refresh daily and monthly statistics tables. These provide MIN(), MAX(), and AVG() for all turbines over daily and monthly periods.\n",
    "\n",
    "These tables serve as the \"Gold\" layer and can be used for reporting and analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30cea029-b9f0-46aa-b324-5e1f7d2ebf8b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Daily Aggregates"
    }
   },
   "outputs": [],
   "source": [
    "def dailyAggregates(data_df):\n",
    "    daily_df = data_df.groupBy(\"turbine_id\", \"date\") \\\n",
    "                    .agg(\n",
    "                        min(\"power_output\").alias(\"min_output\"),\n",
    "                        max(\"power_output\").alias(\"max_output\"),\n",
    "                        avg(\"power_output\").alias(\"avg_output\")\n",
    "                    ) \\\n",
    "\n",
    "    daily_df.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .clusterBy(\"turbine_id\", \"date\") \\\n",
    "            .saveAsTable(\"lakehouse_sbx.cd_edw.turbine_stats_daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a8323f5-497c-4e36-ac2d-0349e9fc8536",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Monthly Aggregates"
    }
   },
   "outputs": [],
   "source": [
    "def monthlyAggregates(data_df):\n",
    "    monthly_df = data_df.groupBy(\"turbine_id\", \"month\") \\\n",
    "                    .agg(\n",
    "                        min(\"power_output\").alias(\"min_output\"),\n",
    "                        max(\"power_output\").alias(\"max_output\"),\n",
    "                        avg(\"power_output\").alias(\"avg_output\")\n",
    "                    ) \n",
    "\n",
    "    monthly_df.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .clusterBy(\"turbine_id\", \"month\") \\\n",
    "            .saveAsTable(\"lakehouse_sbx.cd_edw.turbine_stats_monthly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ded7a459-f906-477f-8dbb-6d03d1ab50f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Anomalies Table"
    }
   },
   "outputs": [],
   "source": [
    "def anomaliesTable(data_df):\n",
    "    anomalies_df = data_df.where(data_df.is_24h_anomaly == True)\n",
    "\n",
    "    return(anomalies_df.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .clusterBy(\"turbine_id\", \"month\") \\\n",
    "            .saveAsTable(\"lakehouse_sbx.cd_edw.turbine_24h_anomalies\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "159214c5-1d46-4d20-a39c-ba729a1012e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Simple Orchestrator\n",
    "Bring together and execute all functions defined above in one location. This notebook can be executed by a Job/Workflow on Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ff4d9d2-ebdd-4f55-a39f-9c24bcba6d02",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Simple Orchestrator"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    load_csv = load_csv_incremental(schema, '/Volumes/lakehouse_sbx/cd_edw/landing/cd_raw_data/')\n",
    "    impute_data = imputeMissingData()\n",
    "    calc_stats = calculateStats()\n",
    "    data_df = spark.table(\"lakehouse_sbx.cd_edw.slvr_turbine_data_02\")\n",
    "    daily_agg = dailyAggregates(data_df)\n",
    "    monthly_agg = monthlyAggregates(data_df)\n",
    "    anomalies_table = anomaliesTable(data_df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4688404763683774,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Load_Transform_Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
